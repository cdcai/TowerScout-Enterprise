# Databricks notebook source
# MAGIC %md
# MAGIC # Notebook Overview
# MAGIC
# MAGIC ## Purpose
# MAGIC This notebook is designed to facilitate the re-training of the ML models used in the infererence.
# MAGIC
# MAGIC ## Inputs
# MAGIC - **batch_size**: The batch size used to train the model (default:1)
# MAGIC - **epochs**: Number of epochs to train the model (default:5)
# MAGIC - **max_evals**: The maximum number of evaluations (default:16)
# MAGIC - **metrics**: Metric to evaluate the model after training (default: MSE)
# MAGIC - **source_schema**: 
# MAGIC - **source_table**:
# MAGIC
# MAGIC ## Processes
# MAGIC - Running the utility scripts to perform data processing tasks.
# MAGIC
# MAGIC ## Outputs
# MAGIC - Processed data results generated by the utility scripts.

# COMMAND ----------

# MAGIC %run ./utils

# COMMAND ----------

# MAGIC %run ./utils

# COMMAND ----------

# MAGIC %run ./demo_model

# COMMAND ----------

# MAGIC %run ./dataloader_development

# COMMAND ----------

# Purpose: This cell imports necessary libraries and modules for machine learning, 
# hyperparameter optimization, and data handling in a Spark environment.

import mlflow
from mlflow.models.signature import infer_signature
from mlflow import MlflowClient

from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK, base
from hyperopt.pyll import scope

from functools import partial  # For passing extra arguments to objective function
from dataclasses import dataclass, asdict, field
from collections import namedtuple

from typing import Any

from enum import Enum

from torch import nn

from petastorm.spark.spark_dataset_converter import SparkDatasetConverter

# COMMAND ----------

# Purpose: This cell sets up logging for the application, including creating a rotating file handler 
# and configuring the logging format. It also initializes the DBUtils object for interacting with Databricks utilities.

import logging
from logging.handlers import RotatingFileHandler
import os
from pyspark.dbutils import DBUtils
import tempfile

# Create a DBUtils object
dbutils = DBUtils(spark.sparkContext)

# Set up your log path
temp_file = tempfile.NamedTemporaryFile(delete=False)
log_path = temp_file.name

# Set up your log path
dbfs_log_path = "/Volumes/edav_dev_csels/towerscout_test_schema/test_volume/logs/towerscout.log"

# Create the log directory if it doesn't exist
log_dir = os.path.dirname(log_path)

# Set up logging
logger = logging.getLogger('towerscout')
logger.setLevel(logging.INFO)
logger.handlers.clear()

try:
    # Create a rotating file handler
    handler = RotatingFileHandler(log_path, maxBytes=1000000, backupCount=1)
    handler.setLevel(logging.INFO)

    # Create a logging format
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)

    # Add the handler to the logger
    logger.addHandler(handler)
except Exception as e:
    print(f"Error setting up logging: {e}")

# Now you can use the logger
logger.info('This is an info message and the first message of the logs.')
logger.warning('This is a warning message.')
logger.error('This is an error message.')

# COMMAND ----------

# DBTITLE 1,Data Ingestion functions
# Purpose: This cell defines two functions to split a Spark DataFrame into train, test, and validation sets.
# The first function handles DataFrames with labels, while the second handles DataFrames without labels.

def split_data(images: DataFrame) -> (DataFrame, DataFrame, DataFrame):
    """
    Splits a Spark dataframe into train, test, and validation sets.

    Args:
        images (DataFrame): Input dataframe to be split.

    Returns:
        tuple: A tuple containing the train, test, and validation dataframes.
    """
    # Split the dataframe into train set (80% of each label)
    images_train = images.sampleBy(("label"), fractions={0: 0.8, 1: 0.8})
    
    # Get the remaining data after excluding the train set
    images_remaining = images.join(images_train, on='path', how='leftanti')
    
    # Split the remaining data into validation set (50% of remaining data)
    images_val = images_remaining.sampleBy(("label"), fractions={0: 0.5, 1: 0.5})
    
    # The rest of the data is the test set
    images_test = images_remaining.join(images_val, on='path', how='leftanti')
    
    logger.info("Splitting data into train, test, and validation sets")
    return images_train, images_test, images_val


def split_datanolabel(images: DataFrame) -> (DataFrame, DataFrame, DataFrame):
    """
    Splits a Spark dataframe into train, test, and validation sets.

    Args:
        images (DataFrame): Input dataframe to be split.

    Returns:
        tuple: A tuple containing the train, test, and validation dataframes.
    """
    # Split the dataframe into train set (80% of data)
    images_train = images.sample(fraction=0.8)
    
    # Get the remaining data after excluding the train set
    images_remaining = images.join(images_train, on='path', how='leftanti')
    
    # Split the remaining data into validation set (50% of remaining data)
    images_val = images_remaining.sample(fraction=0.5)
    
    # The rest of the data is the test set
    images_test = images_remaining.join(images_val, on='path', how='leftanti')
    
    logger.info("Splitting data into train, test, and validation sets (dataframe does not have label)")
    return images_train, images_test, images_val

# COMMAND ----------

class ValidMetric(Enum):
    BCE = nn.BCEWithLogitsLoss()
    MSE = nn.MSELoss()

dbutils.widgets.text("source_schema", defaultValue="towerscout_test_schema")
dbutils.widgets.text("source_table", defaultValue="image_metadata")

dbutils.widgets.text("epochs", defaultValue="5")
dbutils.widgets.text("batch_size", defaultValue="1")
dbutils.widgets.text("report_interval", defaultValue="5")
dbutils.widgets.text("max_evals", defaultValue="16")
dbutils.widgets.text("parallelism", defaultValue="4")

stages = ["Dev", "Staging", "Production"]
dbutils.widgets.dropdown("stage", "Production", stages)

metrics = [member.name for member in ValidMetric]
dbutils.widgets.dropdown("objective_metric", "MSE", metrics)
dbutils.widgets.multiselect("metrics", "MSE" , choices=metrics)

# COMMAND ----------

# DBTITLE 1,Retrieve parameters from notebook
# Purpose: This cell retrieves and processes widget parameters for model training configuration.

# Get the selected objective metric from the widget
objective_metric = dbutils.widgets.get("objective_metric")

# Get the number of epochs from the widget and convert to integer
epochs = int(dbutils.widgets.get("epochs"))

# Get the batch size from the widget and convert to integer
batch_size = int(dbutils.widgets.get("batch_size"))

# Get the report interval from the widget and convert to integer
report_interval = int(dbutils.widgets.get("report_interval"))

# Get the list of metrics from the widget, split by comma, and convert to ValidMetric enum
metrics = [ValidMetric[metric] for metric in dbutils.widgets.get("metrics").split(",")]

# Get the parallelism level from the widget and convert to integer
parallelism = int(dbutils.widgets.get("parallelism"))

# Get the maximum number of evaluations from the widget and convert to integer
max_evals = int(dbutils.widgets.get("max_evals"))

# Log that parameters have been loaded
logger.info("Loaded parameters.")

# COMMAND ----------

# Purpose: This cell sets up the MLflow registry URI to use Databricks Unity Catalog (UC) and creates an MLflow client.

# Set registry to be UC model registry
mlflow.set_registry_uri("databricks-uc")

# Create MLflow client
client = MlflowClient()
logger.info("Created MLflow client.")

# COMMAND ----------

# DBTITLE 1,Data Ingestion and Splitting
# Purpose: This cell retrieves catalog and schema information, constructs the table name, 
# loads image data from the specified table, and splits the data into training, testing, and validation sets.

# Retrieve catalog information from Spark configuration
catalog_info = CatalogInfo.from_spark_config(spark) # CatalogInfo class defined in utils nb
catalog = catalog_info.name

# Get schema and source table from widgets
schema = dbutils.widgets.get("source_schema")
source_table = dbutils.widgets.get("source_table")

# Construct the full table name
table_name = f"{catalog}.{schema}.{source_table}"

# Load image data from the specified table
images = (
    spark
    .table(table_name)
    .select("content", "path") 
)

# Split the data into training, testing, and validation sets
train_set, test_set, val_set = split_datanolabel(images)
logger.info("Loaded data.")

# COMMAND ----------

# Purpose: This cell counts the number of records in the training, validation, and testing sets 
# and displays the counts using the display function.

# Count and display the number of records in the training set
display(train_set.count())

# Count and display the number of records in the validation set
display(val_set.count())

# Count and display the number of records in the testing set
display(test_set.count())

# COMMAND ----------

# DBTITLE 1,Data classes and named tuples
# Purpose: Define argument classes and named tuples for hyperparameter tuning, 
# model training, and model promotion, including their attributes and default values.

FminArgs = namedtuple('FminArgs', ['fn', 'space', 'algo', 'max_evals', 'trials'])

@dataclass
class SplitConverters:
    """
    A class to hold the spark dataset converters for the training, testing,
    and validation sets.

    Attributes:
        train: The spark dataset converter for the training dataset.
        val: The spark dataset converter for the validation dataset.
        test: The spark dataset converter for the testing dataset.
    """
    train: SparkDatasetConverter = None
    val: SparkDatasetConverter = None
    test: SparkDatasetConverter = None

@dataclass
class TrainingArgs:
    """
    A class to represent model training arguments.

    Attributes:
        objective_metric: The evaluation metric we want to optimize.
        epochs: Number of epochs to optimize model over.
        batch_size: The size of the minibatches passed to the model.
        report_interval: Interval to log metrics during training.
        metrics: Various model evaluation metrics we want to track.
    """
    objective_metric: str = "recall"  # Default metric for optimization
    epochs: int = 2
    batch_size: int = 4
    report_interval: int = 5
    metrics: list[ValidMetric] = field(default_factory=dict)

@dataclass
class PromotionArgs:
    """
    A class to represent model promotion arguments.

    Attributes:
        objective_metric: The evaluation metric we want to optimize.
        batch_size: The size of the minibatches passed to the model.
        metrics: Various model evaluation metrics we want to track.
        model_version: The version of the model that is the challenger.
        model_name: The name of the model.
        challenger_metric_value: The value of the objective metric achieved by the challenger model on the test dataset.
        alias: The alias we are promoting the model to.
        test_conv: The converter for the test dataset.
    """
    objective_metric: str = "recall"
    batch_size: int = 4
    metrics: list[ValidMetric] = field(default_factory=dict)
    model_version: int = 1
    model_name: str = "ts"
    challenger_metric_value: float = 0
    alias: str = "staging"
    test_conv: SparkDatasetConverter = None

# COMMAND ----------

# Purpose: This cell defines functions to create a Petastorm converter for a Spark DataFrame, 
# perform a single pass (epoch) over the data using the converter, and train a model with given hyperparameters.

def get_converter_df(dataframe: DataFrame) -> callable:
    """
    Creates a Petastorm converter for a Spark DataFrame.

    Args:
        dataframe: The Spark DataFrame.
    Returns:
        callable: A Petastorm converter.
    """
    dataframe = dataframe.transform(compute_bytes, "content")
    converter = create_converter(dataframe, "bytes")
    logger.info(f"Creating converter for {dataframe.count()} rows")
    return converter

def perform_pass(
        model_trainer: ModelTrainer, 
        converter: callable, 
        context_args: dict[str, Any], 
        train_args: TrainingArgs, 
        mode: str, 
        epoch_num: int = 0
    ) -> dict[str, float]:
    """
    Performs a single pass (epoch) over the data accessed by the converter.

    Args:
        model_trainer: The model trainer.
        converter: The Petastorm converter.
        context_args: Arguments for the converter context.
        train_args: Contains training arguments such as batch size.
        mode: Specifies if the model is in training or evaluation mode.
        epoch_num: The current epoch number for logging metrics across epochs.
    Returns:
        dict[str, float]: A dict containing values of various metrics for the epoch.
    """
    metrics = {}
    converter_length = len(converter)
    steps_per_epoch = converter_length // train_args.batch_size
    report_interval = train_args.report_interval if mode == "TRAIN" else converter_length

    with converter.make_torch_dataloader(**context_args) as dataloader:
        dataloader_iter = iter(dataloader)
        for minibatch_num in range(steps_per_epoch):
            minibatch_images = next(dataloader_iter)
            if mode == "TRAIN":
                metrics = model_trainer.training_step(minibatch_images, mode)
            else:
                metrics = model_trainer.validation_step(minibatch_images, mode)
            if minibatch_num % report_interval == 0:
                is_train = mode == "TRAIN"
                mlflow.log_metrics(metrics, step=is_train * (minibatch_num + epoch_num * converter_length))
    logger.info(f"Epoch {epoch_num} completed")
    return metrics

def train(
        params: dict[str, Any], 
        train_args: TrainingArgs,
        split_convs: SplitConverters
    ) -> dict[str, Any]:
    """
    Trains a model with given hyperparameter values and returns the value 
    of the objective metric on the validation dataset.

    Args:
        params: The hyperparameter values to train the model with.
        train_args: The arguments for training and validation loops.
        split_convs: The converters for the train/val/test datasets.
    Returns:
        dict[str, float]: A dict containing the loss.
    """
    with mlflow.start_run(nested=True):
        # Create model and trainer
        model_trainer = ModelTrainer(optimizer_args=params, metrics=train_args.metrics)
        mlflow.log_params(params)
        
        context_args = {
            "transform_spec": get_transform_spec(),
            "batch_size": train_args.batch_size
        }
        
        # Training
        for epoch in range(train_args.epochs):
            train_metrics = perform_pass(model_trainer, split_convs.train, context_args, train_args, "TRAIN", epoch)
   
        # Validation
        for epoch in range(train_args.epochs):
            val_metrics = perform_pass(model_trainer, split_convs.val, context_args, train_args, "VAL", epoch) 

        # Testing     
        test_metrics = perform_pass(model_trainer, split_convs.test, context_args, train_args, "TEST")

        with split_convs.test.make_torch_dataloader(**context_args) as dataloader:
            dataloader_iter = iter(dataloader)
            images = next(dataloader_iter)  # To get model signature
            signature = infer_signature(
                model_input=images['features'].numpy(), 
                model_output=model_trainer.forward(images).logits.detach().numpy()
            )
            mlflow.pytorch.log_model(model_trainer.model, "ts-model-mlflow", signature=signature)
        
        metric = val_metrics[f"{train_args.objective_metric}_VAL"]  # Minimize loss on val set because we are tuning hyperparams
    logger.info(f"Best model has {train_args.objective_metric} of {metric}")
    # Set the loss to -1*f1 so fmin maximizes the f1_score
    return {'status': STATUS_OK, 'loss': -1 * metric}

# COMMAND ----------

# Purpose: This function tunes hyperparameters using HyperOpt's fmin function, 
# logs the best run in MLflow, and returns the best run, its test metric, and the best parameters.

def tune_hyperparams(
                    fmin_args: FminArgs, 
                     train_args: TrainingArgs
    ) -> tuple[Any, float, dict[str, Any]]:
    """
    Returns the best MLflow run and testing value of objective metric for that run

    Args:
        fmin_args: FminArgs The arguments to HyperOpts fmin function
        train_args: TrainingArgs The arguements for training and validaiton loops
    """
    with mlflow.start_run(run_name='towerscout_retrain'):
        # Perform hyperparameter tuning using HyperOpt's fmin function
        best_params = fmin(**(fmin_args._asdict())) # cant pass raw dataclass using **, must be mappable (dict)
      
    # Sort by validation objective metric in descending order (assuming higher is better)
    best_run = mlflow.search_runs(order_by=[f'metrics.{train_args.objective_metric + "_VAL"} DESC']).iloc[0]
    
    # Get test score of the best run
    best_run_test_metric = best_run[f"metrics.{train_args.objective_metric}_TEST"]
    
    # End the MLflow run
    mlflow.end_run()
    
    logger.info(f"Best model has {train_args.objective_metric} of {best_run_test_metric}")
    
    return best_run, best_run_test_metric, best_params

# COMMAND ----------

# MAGIC %md
# MAGIC Finish testing model promo logic
# MAGIC Add doc strings and type hints to funcs
# MAGIC Group funcs into seprate notebooks

# COMMAND ----------

# Purpose: This cell creates Petastorm converters for train/val/test Spark DataFrames, 
# sets up training arguments and hyperparameter search space, and prepares for hyperparameter tuning using HyperOpt.

# Create converters for train/val/test Spark DataFrames
converter_train = get_converter_df(train_set)
converter_val = get_converter_df(val_set)
converter_test = get_converter_df(test_set)

# Define training arguments
train_args = TrainingArgs(
    objective_metric=objective_metric, 
    epochs=epochs, 
    batch_size=batch_size, 
    report_interval=report_interval, 
    metrics=metrics
)

# Define converters for train/val/test datasets
split_convs = SplitConverters(
    train=converter_train, 
    val=converter_val, 
    test=converter_test
)

# Set up SparkTrials for parallel hyperparameter tuning
trials = SparkTrials(parallelism=parallelism)
logger.info(f'Training with {train_args.epochs} epochs, {train_args.batch_size} batch size, {train_args.report_interval} report interval, {train_args.objective_metric} objective metric')

# Define hyperparameter search space
search_space = {
    'lr': hp.uniform('lr', 1e-4, 1e-3), 
    'weight_decay': hp.uniform('weight_decay', 1e-4, 1e-3)
}

# Use partial to pass extra arguments to the objective function
objective_func = partial(
    train, 
    train_args=train_args, 
    split_convs=split_convs
)

# Define arguments for HyperOpt's fmin function
fmin_args = FminArgs(
    fn=objective_func,
    space=search_space,
    algo=tpe.suggest,
    max_evals=max_evals,
    trials=trials
)

# COMMAND ----------

# Purpose: This cell performs hyperparameter tuning using the tune_hyperparams function,
# handles potential exceptions, and logs the results.

try:
    # Perform hyperparameter tuning and retrieve the best run, its test metric, and the best parameters
    best_run, challenger_test_metric, best_params = tune_hyperparams(fmin_args, train_args)
    logger.debug("Hyperparameter tuning completed.")
except ValueError as e:
    # Handle invalid hyperparameter tuning arguments
    logger.error(f"Invalid hyperparameter tuning arguments: {e}")
except RuntimeError as e:
    # Handle runtime errors during hyperparameter tuning
    logger.error(f"Error during hyperparameter tuning: {e}")
except Exception as e:
    # Handle any other unexpected errors during hyperparameter tuning
    logger.error(f"Unexpected error during hyperparameter tuning: {e}")

# COMMAND ----------

# Purpose: This cell prints the testing metric value of the best run using the specified objective metric.

display(f'Testing metric ({train_args.objective_metric}) value of best run: {challenger_test_metric}')

# COMMAND ----------

# DBTITLE 1,Register challenger model
# Purpose: This cell registers the best model from the hyperparameter tuning run in the MLflow model registry,
# assigns an alias to the registered model, and logs the registration details.

# Get the run ID of the best run
run_id = best_run.run_id

# Define the model name using catalog and schema
model_name = f"{catalog}.{schema}.towerscout_model"

# Retrieve the alias for the model stage from Databricks widgets
alias = dbutils.widgets.get("stage")

# Register the model in MLflow using the run ID and model URI
challenger_model_metadata = mlflow.register_model(
    model_uri=f"runs:/{run_id}/ts-model-mlflow",  # Path to the logged artifact folder called models
    name=model_name  # Name for the model in the catalog
)

# Log the registration details
logger.info(f"Registered model {model_name} with version {challenger_model_metadata.version}")

# COMMAND ----------

# Purpose: This cell sets the registered model alias for the challenger model, 
# effectively promoting it if it outperforms the current production model.

# Set the registered model alias for the challenger model
client.set_registered_model_alias(
    name=challenger_model_metadata.name, 
    alias=alias, 
    version=challenger_model_metadata.version  # Get version of challenger model from when it was registered
)

# COMMAND ----------

promo_args = PromotionArgs(
    objective_metric=train_args.objective_metric,
    batch_size=train_args.batch_size,
    metrics=train_args.metrics,
    model_version=challenger_model_metadata.version,
    model_name=model_name,
    challenger_metric_value=challenger_test_metric,
    alias=alias,
    test_conv=split_convs.test
    )

# COMMAND ----------

# DBTITLE 1,Model Promotion Logic
def model_promotion(promo_args: PromotionArgs) -> None:
    """
    Evaluates the model that has the specficied alias. Promotes the model with the
    specfied

    Args:
        promo_args: Contains arguments for the model promotion logic
    Returns:
        None
    """

    # load current model with matching alias (champion model)
    champ_model = mlflow.pytorch.load_model(
        model_uri=f"models:/{promo_args.model_name}@{promo_args.alias}"
        )
    
    #model_trainer = TowerScoutModelTrainer(optimizer_args={"lr": 0}, metrics=train_args.metrics)
    # use dummy optimzer params since optimizer is irrelevant for inference
    model_trainer = ModelTrainer(optimizer_args={"lr": 0}, metrics=promo_args.metrics)
    model_trainer.model = champ_model # Replace initial model with prod model

    context_args = {
                "transform_spec": get_transform_spec(),
                "batch_size": promo_args.batch_size
            }

    # get testing score for current produciton model
    steps_per_epoch = len(promo_args.test_conv) // promo_args.batch_size
    with promo_args.test_conv.make_torch_dataloader(**context_args) as dataloader:
        dataloader_iter = iter(dataloader)
        for minibatch_num in range(steps_per_epoch):
            minibatch_images = next(dataloader_iter)
            champ_model_test_metrics = model_trainer.validation_step(minibatch_images, "TEST")

    champ_test_metric = champ_model_test_metrics[f"{promo_args.objective_metric}_TEST"]
    print(f"{promo_args.objective_metric} for production model is: {champ_test_metric}")

    if challenger_test_metric > champ_test_metric:
        print(f"Promoting challenger model to {alias}.")
        # give alias to challenger model, alias is automatically removed from current champion model
        client.set_registered_model_alias(
            name=promo_args.model_name, 
            alias=promo_args.alias, 
            version=promo_args.model_version # version of challenger model from when it was registered
        )
    else:
        print("Challenger model performs worse than current production model. Promotion aborted.")
    

# COMMAND ----------

# Purpose: Attempt to promote the challenger model using the provided promotion arguments.
# If an error occurs during the promotion process, log the error.

try:
    model_promotion(promo_args)
    logger.debug("Promotion completed.")
except Exception as e:
    logger.error(f"Error during model promotion: {e}")

# COMMAND ----------

# DBTITLE 1,Delete converters
# Purpose: Delete the training, validation, and test data converters to free up resources.

# Delete the training data converter
converter_train.delete()

# Delete the validation data converter
converter_val.delete()

# Delete the test data converter
converter_test.delete()

# COMMAND ----------

# Purpose: Close the logger handler, upload the logs to DBFS, and read the contents of the log file from DBFS.

# Close the handler to ensure the file is properly closed
handler.close()
logger.removeHandler(handler)

# Upload logs to DBFS
def upload_logs_to_dbfs():
    try:
        # Use dbutils to upload the log file to DBFS
        dbutils.fs.cp(f"file:{log_path}", dbfs_log_path)
        logger.debug("Logs uploaded to DBFS")
    except Exception as e:
        logger.error(f"Error uploading logs to DBFS: {e}")

# Call the upload function
upload_logs_to_dbfs()

# Read the contents of the file from DBFS
try:
    dbfs_contents = dbutils.fs.head(dbfs_log_path)
    display(dbfs_contents)
except Exception as e:
    logger.error(f"Error reading logs from DBFS: {e}")
