{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ee030e-c506-488d-82ff-047937cafbce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Benchmark table schema\n",
    "\n",
    "model_metadata: STRUCT<uc_model_name: STRING, uc_model_version: INT, model_uri: STRING >\n",
    "\n",
    "overall_metrics: STRUCT<\"f1\": float, \"recall\": float, \"precision\": float>\n",
    "\n",
    "per_class_metrics: STRUCT<\"ct\": STRUCT<\"f1\": float, \"recall\": float, \"precision\": float>>\n",
    "\n",
    "benchmarked_at: timestamp\n",
    "\n",
    "# ASSUMPTIONS\n",
    "Only models from the new Ultralytics lib will be benchmarked\n",
    "\n",
    "Benchmark dataset does not change frequently (so conversion to MDS files isn't to cumbersome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6b2d2f-318a-4a53-8af1-8f10abd339e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import mlflow\n",
    "from tsdb.ml.types import Hyperparameters\n",
    "from tsdb.ml.utils import UCModelName\n",
    "from tsdb.ml.datasets import get_dataloader\n",
    "from tsdb.preprocessing.preprocess import convert_to_mds\n",
    "from ultralytics.nn.tasks import attempt_load_one_weight, DetectionModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tsdb.ml.validate import ModifiedDetectionValidator\n",
    "\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34553b33-f013-4074-bcba-17921a50fcb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format(\"delta\")\n",
    "        .table(\"edav_dev_csels.towerscout.benchmark_scored\")\n",
    "        .selectExpr(\n",
    "            \"substring(path, 6, length(path)) as image_path\",  # remove 'dbfs:' prefix on image paths\n",
    "            \"results.bboxes as 2D_bboxes\", \n",
    "            \"transform(2D_bboxes, x -> float(0)) AS cls\", \n",
    "            \"flatten(transform(2D_bboxes, x -> array(x.x1, x.y1, x.x2, x.y2))) AS bboxes\",  # flatten bboxes to align with mds data types\n",
    "        )\n",
    "    )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706e1b7d-8358-403f-8e48-dc6aff8e83cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.udf(T.StringType())\n",
    "def get_location(file_path: str) -> str:\n",
    "\n",
    "    img_filename = file_path.split(\"/\")[-1]\n",
    "    nyc_path = \"/Volumes/edav_dev_csels/towerscout/images/okr1/benchmark/\"\n",
    "    denver_path = \"/Volumes/edav_dev_csels/towerscout/images/okr1/benchmark-denver/\"\n",
    "\n",
    "    if os.path.isfile( os.path.join(nyc_path, img_filename) ):\n",
    "        return \"nyc\"\n",
    "    elif os.path.isfile( os.path.join(denver_path, img_filename) ):\n",
    "        return \"denver\"\n",
    "    else:\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a082cb38-90e9-42f5-bc6c-0b37c1d9adda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_locations = df.withColumn(\"location\", get_location(F.col('image_path')))\n",
    "display(df_locations.groupBy(\"location\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8313c456-dd1e-412a-9b2e-0eb5a11bfada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert_to_mds(df_locations.filter(\"location = 'denver'\"), \"/Volumes/edav_dev_csels/towerscout/data/mds_benchmark/denver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a3954d-1177-4e32-a76b-53d5541a7c29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_model(\n",
    "    model: DetectionModel, mds_benchmark_dir: str, cache_dir: str, batch_size: int\n",
    ") -> tuple[dict[str, float], dict[str, dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    This function takes a YOLO model from the newer Ultralytics library and benchmarks it against the MDS dataset\n",
    "    at the directory supplied. To do this we use the ModifiedDetectionValidator object from the tsdb library.\n",
    "\n",
    "    Args:\n",
    "        model: the DetectionModel we want to benchmark\n",
    "        mds_benchmark_dir: the directory containing the MDS files of the benchmark dataset\n",
    "        cache_dir: the directory for the dataloader to use\n",
    "        batch_size: the batch size for the dataloader\n",
    "\n",
    "    Returns:\n",
    "        overall_metrics: a dictionary of the overall metrics (f1, precision, recall) for the model\n",
    "        per_class_metrics: a dictionary of the metrics (f1, precision, recall) for each class for the model\n",
    "    \"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    dataloader = get_dataloader(\n",
    "        local_dir=cache_dir,\n",
    "        remote_dir=mds_benchmark_dir,\n",
    "        hyperparams=Hyperparameters(batch_size=batch_size),\n",
    "        split=None,\n",
    "        transform=False,\n",
    "    )\n",
    "\n",
    "    validator = ModifiedDetectionValidator(\n",
    "        dataloader=dataloader, training=False, device=device, args=copy(model.args)\n",
    "    )\n",
    "\n",
    "    metrics = validator(model)\n",
    "    per_class = validator.metrics.box\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"f1\": metrics[\"metrics/f1(B)\"],\n",
    "        \"precision\": metrics[\"metrics/precision(B)\"],\n",
    "        \"recall\": metrics[\"metrics/recall(B)\"],\n",
    "    }\n",
    "\n",
    "    per_class_metrics = {\n",
    "        class_name: {\n",
    "            \"f1\": per_class.f1[class_label],\n",
    "            \"precision\": per_class.p[class_label],\n",
    "            \"recall\": per_class.r[class_label],\n",
    "        }\n",
    "        for class_label, class_name in model.names.items()\n",
    "    }\n",
    "\n",
    "    return overall_metrics, per_class_metrics\n",
    "\n",
    "\n",
    "def update_benchmark_table(\n",
    "    benchmark_table: str,\n",
    "    overall_metrics: dict[str, float],\n",
    "    per_class_metrics: dict[str, dict[str, float]],\n",
    "    model_metadata: dict,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function updates the benchmark table with the supplied overall metrics, per class\n",
    "    metrics and model metadata for a benchmark run of the model.\n",
    "\n",
    "    Args:\n",
    "        benchmark_table: the name of the benchmark table to update in the format:\n",
    "                         {catalog}.{schema}.{table_name}\n",
    "        overall_metrics: a dictionary of the overall metrics (f1, precision, recall) for the model\n",
    "        per_class_metrics: a dictionary of the metrics (f1, precision, recall) for each class for the model\n",
    "        model_metadata: a dictionary of the model metadata (uc_model_name, uc_model_version, model_uri) to include in the benchmark table\n",
    "    \"\"\"\n",
    "\n",
    "    # must cast to float from numpy.float64 because PySpark FloatType can't take numpy floats.\n",
    "    overall_metrics = {key: float(value) for key, value in overall_metrics.items()}\n",
    "    per_class_metrics = {\n",
    "        key: {k: float(v) for k, v in value.items()}\n",
    "        for key, value in per_class_metrics.items()\n",
    "    }\n",
    "\n",
    "    data = [(model_metadata, None, overall_metrics, per_class_metrics)]\n",
    "\n",
    "    schema = spark.read.format(\"delta\").table(benchmark_table).schema\n",
    "\n",
    "    new_row_df = spark.createDataFrame(data, schema)\n",
    "    new_row_df = new_row_df.withColumn(\"benchmarked_at\", F.current_timestamp())\n",
    "    new_row_df.write.format(\"delta\").mode(\"append\").saveAsTable(benchmark_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb59eef-2bb8-49d7-8652-ffd6edc1f824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bm_df = spark.read.format(\"delta\").table(\"edav_dev_csels.towerscout.benchmark_results\")\n",
    "bm_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9b39660-ca91-4bf4-a805-f26abad779df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "\n",
    "# USE CATALOG edav_dev_csels;\n",
    "# USE SCHEMA towerscout;\n",
    "\n",
    "# CREATE TABLE IF NOT EXISTS benchmark_results (\n",
    "#   model_metadata STRUCT<uc_model_name: STRING, uc_model_version: INT, model_uri: STRING >,\n",
    "#   benchmarked_at TIMESTAMP,\n",
    "#   overall_metrics STRUCT<f1: FLOAT, recall: FLOAT, precision: FLOAT>,\n",
    "#   per_class_metrics STRUCT<ct: STRUCT<f1: FLOAT, recall: FLOAT, precision: FLOAT>>\n",
    "#   )\n",
    "#   USING delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf3b1ea-fbcc-4ff7-8769-5b2795d68457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(\n",
    "    \"runs:/ec50663136d5413b89b1600e6d66c7f2/towerscout_model\", map_location=device\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "cache_dir = \"/local/cache/path1\"\n",
    "\n",
    "mds_benchmark_dir = \"/Volumes/edav_dev_csels/towerscout/data/mds_benchmark/nyc\"\n",
    "\n",
    "overall_metrics, per_class_metrics = benchmark_model(model=model, mds_benchmark_dir=mds_benchmark_dir, cache_dir=cache_dir, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3050a732-6dc2-4ef4-9b74-f7497c306181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(overall_metrics, per_class_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97dff29-7793-4a12-a692-49fb19d08fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_metadata = {\"uc_model_name\": \"Model A\", \"uc_model_version\": 1, \"model_uri\": \"s3://models/model_a\"}\n",
    "\n",
    "display(update_benchmark_table(\"edav_dev_csels.towerscout.benchmark_results\", overall_metrics, per_class_metrics, model_metadata))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6807102834957750,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "benchmark_dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
