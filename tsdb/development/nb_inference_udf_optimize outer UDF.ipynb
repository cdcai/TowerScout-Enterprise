{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b09f0e-76be-4f2f-930f-5b6cb8a6e328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from json import loads\n",
    "from typing import Any, TypedDict, Union, Iterable\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tsdb.ml.utils import cut_square_detection\n",
    "import tsdb.preprocessing.transformations as trf\n",
    "from tsdb.preprocessing.images import make_image_metadata_udf\n",
    "from tsdb.ml.infer import make_towerscout_predict_udf\n",
    "from tsdb.ml.detections import YOLOv5_Detector\n",
    "from tsdb.ml.efficientnet import EN_Classifier\n",
    "from models.common import Detections  # Detection object for YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f175f2-a5b7-4e8d-80f3-66fbd97e9391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ImageMetadata(TypedDict):\n",
    "    \"\"\"\n",
    "    A class to represent image metadata.\n",
    "    height: the image height\n",
    "    width: the image width\n",
    "    lat: the latitude of the image\n",
    "    long: the longitude of the image\n",
    "    image_id: the id of the image\n",
    "    map_provider: the map provider the image is from \n",
    "    image: The PIL image object\n",
    "    \"\"\"\n",
    "    height: int\n",
    "    width: int\n",
    "    lat: float\n",
    "    long: float\n",
    "    image_id: int\n",
    "    map_provider: str\n",
    "    image: Image\n",
    "\n",
    "\n",
    "def get_image_metadata(image_binary: bytes) -> ImageMetadata:  # pragma: no cover\n",
    "        # Try to read the image and if we fail, we have to default to\n",
    "        # to the null image case\n",
    "        image_binary = BytesIO(image_binary)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_binary)\n",
    "            exif = image._getexif()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            exif = None\n",
    "        except UnicodeDecodeError:\n",
    "            exif = None\n",
    "\n",
    "        user_comment_exif_id = 37510\n",
    "\n",
    "        if exif is None or user_comment_exif_id not in exif:\n",
    "            # we need to return with default values\n",
    "            fake_image = Image.new('RGB', (640, 640), (0, 0, 0))\n",
    "            return {\n",
    "                \"height\": 640,\n",
    "                \"width\": 640,\n",
    "                \"lat\": 0.0,\n",
    "                \"long\": 0.0,\n",
    "                \"image_id\": -1,\n",
    "                \"map_provider\": \"unknown\",\n",
    "                \"image\": fake_image\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            user_comment_exif = exif[user_comment_exif_id]\n",
    "            exif_dict = loads(\n",
    "                user_comment_exif.decode(\"utf-8\").replace(\"\\'\", \"\\\"\")\n",
    "            )\n",
    "        \n",
    "        except UnicodeDecodeError as e:\n",
    "            # can we gracefully handle this?\n",
    "            raise ValueError(f\"Unable to decode exif data: {e}\")\n",
    "        \n",
    "        image_id = -1 if \"id\" not in exif_dict else int(exif_dict[\"id\"])\n",
    "        return {\n",
    "            \"height\": image.height,\n",
    "            \"width\": image.width,\n",
    "            \"lat\": exif_dict[\"lat\"],\n",
    "            \"long\": exif_dict[\"lng\"],\n",
    "            \"image_id\": image_id,\n",
    "            \"map_provider\": exif_dict[\"mapProvider\"],\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "\n",
    "class ImageBinaryDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index) -> ImageMetadata:\n",
    "         return get_image_metadata(self.images[index])\n",
    "    \n",
    "\n",
    "def inference_collate_fn(data: list[dict[str, ImageMetadata]]) -> dict[str, Union[Image,dict[str, Any]]]:\n",
    "    batch = defaultdict(list)\n",
    "    for item in data:\n",
    "        batch[\"images\"].append(item.pop(\"image\"))\n",
    "        batch[\"images_metadata\"].append(item)\n",
    "\n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e979a6-c02e-4040-b96c-065beffff209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "request_id = \"be69e91f\"\n",
    "user_id = \"cnu4\"\n",
    "base_path = f\"/Volumes/edav_dev_csels/towerscout/images/maps/bronze/{user_id}/{request_id}\"\n",
    "\n",
    "\n",
    "image_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"binaryFile\")\n",
    "    .load(base_path) # parameterize\n",
    "    .select(\"content\")\n",
    "    .limit(20)\n",
    "    #.repartition(8)\n",
    "    #.withColumn(\"inference\", yolo_inference_udf(F.col(\"content\")))\n",
    ")\n",
    "\n",
    "\n",
    "image_df = image_df.toPandas()\n",
    "image_bins = image_df[\"content\"]\n",
    "\n",
    "bin_dataset = ImageBinaryDataset(image_bins)\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "\n",
    "loader = DataLoader(\n",
    "        bin_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers,\n",
    "        collate_fn=inference_collate_fn \n",
    "    )\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5f663c-ee4a-4529-b883-bf2620c43d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yolo_model = mlflow.pytorch.load_model(\n",
    "            model_uri=f\"models:/edav_dev_csels.towerscout.yolo_autoshape@aws\"\n",
    "        )\n",
    "\n",
    "en_model =  mlflow.pytorch.load_model(\n",
    "            model_uri=f\"models:/edav_dev_csels.towerscout.efficientnet@aws\"\n",
    "        )\n",
    "\n",
    "\n",
    "yolo_model.eval()\n",
    "en_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():  # pragma: no cover\n",
    "    en_model.cuda()\n",
    "    yolo_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d078b05-f95b-4111-9cd3-ed160b97bb71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_yolo_detections(\n",
    "    images: list[Image],\n",
    "    yolo_results: Detections,\n",
    "    secondary_model: torch.nn.Module = None,\n",
    "    **kwargs\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A function to parse the detections from the YOLO model by converting them into a list\n",
    "    of dicts with the following keys:\n",
    "    - x1: the x1 coordinate of the bounding box\n",
    "    - y1: the y1 coordinate of the bounding box\n",
    "    - x2: the x2 coordinate of the bounding box\n",
    "    - y2: the y2 coordinate of the bounding box\n",
    "    - conf: the YOLO model confidence of the detection\n",
    "    - class: the class of the detection\n",
    "    - class_name: the name of the class of the detection\n",
    "    - secondary: the secondary model confidence of the detection (if a secondary model is supplied)\n",
    "\n",
    "    Args:\n",
    "        images: the list of PIL images\n",
    "        yolo_results: the Detections object from the YOLO model\n",
    "        secondary_model: the secondary model used to evaluate the detections\n",
    "        **kwargs: additional keyword arguments to pass to the secondary model\n",
    "    Returns:\n",
    "        A list of dicts with the keys from above.\n",
    "    \"\"\"\n",
    "    parsed_results = []\n",
    "    batch_detections = yolo_results.xyxyn\n",
    "\n",
    "    for image, image_detections in zip(images, batch_detections):\n",
    "        image_detections = image_detections.cpu().numpy().tolist()\n",
    "        \n",
    "        if secondary_model is not None:\n",
    "            apply_secondary_model(secondary_model, image, image_detections, **kwargs)\n",
    "\n",
    "        image_results = [\n",
    "                    {\n",
    "                        \"x1\": item[0],\n",
    "                        \"y1\": item[1],\n",
    "                        \"x2\": item[2],\n",
    "                        \"y2\": item[3],\n",
    "                        \"conf\": item[4],\n",
    "                        \"class\": int(item[5]),\n",
    "                        \"class_name\": yolo_results.names[int(item[5])],\n",
    "                        \"secondary\": item[6] if len(item) > 6 else 1,\n",
    "                    }\n",
    "                    for item in image_detections\n",
    "                ]\n",
    "\n",
    "        parsed_results.append(image_results)\n",
    "\n",
    "    return parsed_results\n",
    "\n",
    "\n",
    "def apply_secondary_model(\n",
    "    secondary_model: torch.nn.Module,\n",
    "    image: Image,\n",
    "    detections: list[np.array],\n",
    "    min_conf: float = 0.25,\n",
    "    max_conf: float = 0.65,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A function to apply the secondary model to the detections from the YOLO model. The function \n",
    "    first crops the image based on the bounding box predicted by the YOLO model, then applies \n",
    "    the secondary model to the cropped image to determine the probablity the image contains a cooling tower\n",
    "    and appends the computed probability to the detection array.\n",
    "\n",
    "    Args:\n",
    "        secondary_model: the secondary model to apply to the cropped image\n",
    "        image: the image to crop\n",
    "        detections: list of the detections from the YOLO model for the input image\n",
    "        min_conf: the minimum confidence to apply the secondary model\n",
    "        max_conf: the maximum confidence to apply the secondary model\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize([456, 456]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.5553, 0.5080, 0.4960), std=(0.1844, 0.1982, 0.2017)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, conf = detection[0:5]\n",
    "\n",
    "        # Use secondary model only for certain confidence range\n",
    "        if conf >= min_conf and conf <= max_conf:\n",
    "            bbox_cropped_image = cut_square_detection(image, x1, y1, x2, y2)\n",
    "\n",
    "            # apply transformations\n",
    "            input = transform(bbox_cropped_image).unsqueeze(0)\n",
    "\n",
    "            if torch.cuda.is_available():  # pragma: no cover\n",
    "                input = input.cuda()\n",
    "\n",
    "            # subtract from 1 because the secondary has class 0 as tower\n",
    "            output = 1 - torch.sigmoid(secondary_model(input).cpu()).item()\n",
    "            p2 = output\n",
    "        elif conf < min_conf:\n",
    "            # set secondary classifier probability to 0\n",
    "            p2 = 0\n",
    "        else:\n",
    "            # if >= max_conf set secondary classifier probability to 1\n",
    "            p2 = 1\n",
    "\n",
    "        detection.append(p2)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1c5e95-e673-4fd0-8a9b-c6f30807d3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.types as T\n",
    "from mlflow import set_registry_uri\n",
    "import pandas as pd\n",
    "\n",
    "from tsdb.ml.utils import get_model_tags\n",
    "\n",
    "\n",
    "YOLO_RETURN_TYPE = T.ArrayType(\n",
    "            T.StructType([\n",
    "                T.StructField(\"x1\", T.FloatType(), True),\n",
    "                T.StructField(\"y1\", T.FloatType(), True),\n",
    "                T.StructField(\"x2\", T.FloatType(), True),\n",
    "                T.StructField(\"y2\", T.FloatType(), True),\n",
    "                T.StructField(\"conf\", T.FloatType(), True),\n",
    "                T.StructField(\"class\", T.IntegerType(), True),\n",
    "                T.StructField(\"class_name\", T.StringType(), True),\n",
    "                T.StructField(\"secondary\", T.FloatType(), True),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "\n",
    "MODEL_VERSION_STRUCT = T.StructType([\n",
    "    T.StructField(\"yolo_model\", T.StringType()),\n",
    "    T.StructField(\"yolo_model_version\", T.StringType()),\n",
    "    T.StructField(\"efficientnet_model\", T.StringType()),\n",
    "    T.StructField(\"efficientnet_model_version\", T.StringType())\n",
    "])\n",
    "\n",
    "\n",
    "TOWERSCOUT_IMAGE_METADATA_STRUCT = T.StructType([\n",
    "        T.StructField(\"height\", T.IntegerType()),\n",
    "        T.StructField(\"width\", T.IntegerType()),\n",
    "        T.StructField(\"lat\", T.DoubleType()),\n",
    "        T.StructField(\"long\", T.DoubleType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60344b52-449f-48cc-8a9f-e7ff8ca08d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UDF_RETURN_TYPE = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"bboxes\", YOLO_RETURN_TYPE),\n",
    "        T.StructField(\"model_version\", MODEL_VERSION_STRUCT),\n",
    "        T.StructField(\"image_metadata\", TOWERSCOUT_IMAGE_METADATA_STRUCT),\n",
    "        T.StructField(\"image_id\", T.IntegerType()),\n",
    "        T.StructField(\"map_provider\", T.StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def new_make_towerscout_predict_udf(\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    yolo_alias: str = \"aws\",\n",
    "    efficientnet_alias: str = \"aws\",\n",
    "    batch_size: int = 100,\n",
    "    num_workers: int = 2,\n",
    ") -> callable:\n",
    "    \"\"\"\n",
    "    For a pandas UDF, we need the outer function to initialize the models\n",
    "    and the inner function to perform the inference process. For more\n",
    "    information, see the following reference by NVIDIA:\n",
    "    -\n",
    "\n",
    "    Args:\n",
    "        catalog: The catalog name.\n",
    "        schema: The schema name.\n",
    "        yolo_alias: The alias for the YOLO model in UC.\n",
    "        efficientnet_alias: The alias for the EfficientNet model in UC.\n",
    "        batch_size: Batch size for the DataLoader.\n",
    "        num_workers: Number of workers for the DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        callable: A pandas UDF for inference.\n",
    "    \"\"\"\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    yolo_model_name = f\"{catalog}.{schema}.yolo_autoshape\"\n",
    "    en_model_name = f\"{catalog}.{schema}.efficientnet\"\n",
    "\n",
    "    # Retrieves models by alias\n",
    "    yolo_detector = mlflow.pytorch.load_model(\n",
    "        model_uri=f\"models:/{yolo_model_name}@{yolo_alias}\"\n",
    "    )\n",
    "\n",
    "    en_classifier = mlflow.pytorch.load_model(\n",
    "        model_uri=f\"models:/{en_model_name}@{efficientnet_alias}\"\n",
    "    )\n",
    "\n",
    "    yolo_detector.eval()\n",
    "    en_classifier.eval()\n",
    "\n",
    "    if torch.cuda.is_available():  # pragma: no cover\n",
    "        en_classifier.cuda()\n",
    "        yolo_detector.cuda()\n",
    "\n",
    "    _, yolo_uc_version = get_model_tags(yolo_model_name, yolo_alias)\n",
    "    _, en_uc_version = get_model_tags(en_model_name, efficientnet_alias)\n",
    "\n",
    "    model_metadata = {\n",
    "        \"yolo_model\": \"yolo_autoshape\",\n",
    "        \"yolo_model_version\": yolo_uc_version,\n",
    "        \"efficientnet_model\": \"efficientnet\",\n",
    "        \"efficientnet_model_version\": en_uc_version,\n",
    "    }\n",
    "\n",
    "    @pandas_udf(UDF_RETURN_TYPE)\n",
    "    @torch.no_grad()\n",
    "    def predict(image_bins: pd.Series) -> pd.DataFrame:  # pragma: no cover\n",
    "        \"\"\"\n",
    "        This predict function is distributed across executors to perform inference.\n",
    "\n",
    "        YOLOv5 library expects the following image formats: file, URI, numpy, PIL, OpenCV, torch tensor, multiple.\n",
    "        - See `Autoshape.forward()` in: https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    "        NOTE: Despite this do NOT pass tensors during inference (only training)\n",
    "        - See: https://github.com/ultralytics/yolov5/issues/7030#issuecomment-1078171092\n",
    "\n",
    "        NOTE: No need to resize images for yolov5 lib as it does it for you\n",
    "        - Source: letterbox and exif_transpose funcs in:\n",
    "            https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    "\n",
    "        Args:\n",
    "            image_bins: A partition of the image binaries\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: DataFrame with predicted labels and extracted image metadata.\n",
    "        \"\"\"\n",
    "        bin_dataset = ImageBinaryDataset(image_bins)\n",
    "        loader = DataLoader(\n",
    "            bin_dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=inference_collate_fn,\n",
    "        )\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for batch in loader:\n",
    "            yolo_output = yolo_model(batch[\"images\"])\n",
    "            parsed_results = parse_yolo_detections(\n",
    "                batch[\"images\"], yolo_output, en_classifier\n",
    "            )\n",
    "            # try using zip instead of i\n",
    "            for i in range(len(parsed_results)):\n",
    "                map_provider = batch[\"images_metadata\"][i].pop(\"map_provider\")\n",
    "                image_id = batch[\"images_metadata\"][i].pop(\"image_id\")\n",
    "                \n",
    "                outputs.append(\n",
    "                    {\n",
    "                        \"bboxes\": parsed_results[i],\n",
    "                        \"model_version\": model_metadata,\n",
    "                        \"image_metadata\": batch[\"images_metadata\"][i],\n",
    "                        \"image_id\": image_id,\n",
    "                        \"map_provider\": map_provider,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return pd.DataFrame(outputs) \n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db1da21-0558-42ff-83a8-95027363a3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def old_make_towerscout_predict_udf(\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    yolo_alias: str = \"aws\",\n",
    "    efficientnet_alias: str = \"aws\",\n",
    "    batch_size: int = 100\n",
    ") -> DataFrame:  # pragma: no cover\n",
    "    \"\"\"\n",
    "    For a pandas UDF, we need the outer function to initialize the models\n",
    "    and the inner function to perform the inference. Process. For more\n",
    "    information, see the following reference by NVIDIA:\n",
    "    - \n",
    "\n",
    "    Args:\n",
    "        model_fn (InferenceModelType): The PyTorch model.\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with predictions.\n",
    "    \"\"\" \n",
    "    set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    yolo_model_name = f\"{catalog}.{schema}.yolo_autoshape\" \n",
    "    en_model_name = f\"{catalog}.{schema}.efficientnet\"  \n",
    "\n",
    "    # Retrieves models by alias and create inference objects\n",
    "    yolo_detector = YOLOv5_Detector.from_uc_registry(\n",
    "        model_name=yolo_model_name,\n",
    "        alias=yolo_alias,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # We nearly always use efficientnet for classification but you don't have to\n",
    "    en_classifier = EN_Classifier.from_uc_registry(\n",
    "        model_name=en_model_name,\n",
    "        alias=efficientnet_alias\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "        \"yolo_model\": \"yolo_autoshape\",\n",
    "        \"yolo_model_version\": yolo_detector.uc_version,\n",
    "        \"efficientnet_model\": \"efficientnet\",\n",
    "        \"efficientnet_model_version\": en_classifier.uc_version,\n",
    "    }\n",
    "\n",
    "    return_type = T.StructType([\n",
    "        T.StructField(\"bboxes\", yolo_detector.return_type),\n",
    "        T.StructField(\"model_version\", MODEL_VERSION_STRUCT)\n",
    "    ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(content_series_iter: Iterable[Any]):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        This predict function is distributed across executors to perform inference.\n",
    "\n",
    "        YOLOv5 library expects the following image formats:\n",
    "        For size(height=640, width=1280), RGB images example inputs are:\n",
    "        #   file:        ims = 'data/images/zidane.jpg'  # str or PosixPath\n",
    "        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'\n",
    "        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)\n",
    "        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)\n",
    "        #   numpy:           = np.zeros((640,1280,3))  # HWC\n",
    "        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)\n",
    "        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n",
    "        - Source: https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    "        \n",
    "        The ultralytics lib accepts the following image formats:\n",
    "        - Source: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/model.py \n",
    "        \n",
    "\n",
    "        # No need to resize for yolov5 lib as it does it for you \n",
    "        - Source: letterbox and exif_transpose funcs in:\n",
    "            https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    "\n",
    "        Args:\n",
    "            content_series_iter: Iterator over content series.\n",
    "\n",
    "        Yields:\n",
    "            DataFrame: DataFrame with predicted labels.\n",
    "        \"\"\"\n",
    "        for content_series in content_series_iter:\n",
    "            # Create dataset object to apply transformations\n",
    "            image_batch = [\n",
    "                Image.open(BytesIO(content)).convert(\"RGB\")\n",
    "                for content in content_series\n",
    "            ]\n",
    "\n",
    "            # Perform inference on batch\n",
    "            outputs = yolo_detector.predict(\n",
    "                model_input=image_batch, \n",
    "                secondary=en_classifier\n",
    "            )\n",
    "\n",
    "            outputs = [\n",
    "                {\"bboxes\": output, \"model_version\": metadata}\n",
    "                for output in outputs\n",
    "            ]\n",
    "            yield pd.DataFrame(outputs)\n",
    "\n",
    "\n",
    "    return pandas_udf(return_type, PandasUDFType.SCALAR_ITER)(predict)\n",
    "\n",
    "\n",
    "def old_make_image_metadata_udf(spark: SparkSession):  # pragma: no cover\n",
    "    towerscout_image_metadata_schema = T.StructType([\n",
    "        T.StructField(\"height\", T.IntegerType()),\n",
    "        T.StructField(\"width\", T.IntegerType()),\n",
    "        T.StructField(\"lat\", T.DoubleType()),\n",
    "        T.StructField(\"long\", T.DoubleType()),\n",
    "        T.StructField(\"image_id\", T.IntegerType()),\n",
    "        T.StructField(\"map_provider\", T.StringType())\n",
    "    ])\n",
    "\n",
    "    def get_image_metadata(image_binary: T.BinaryType):  # pragma: no cover\n",
    "        image = Image.open(BytesIO(image_binary))\n",
    "        exif = image._getexif()\n",
    "        user_comment_exif_id = 37510\n",
    "\n",
    "        if exif is None or user_comment_exif_id not in exif:\n",
    "            # we need to return with default values\n",
    "            return {\n",
    "                \"height\": image.height,\n",
    "                \"width\": image.width,\n",
    "                \"lat\": 0.0,\n",
    "                \"long\": 0.0,\n",
    "                \"id\": -1,\n",
    "                \"map_provider\": \"unknown\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            user_comment_exif = exif[user_comment_exif_id]\n",
    "            exif_dict = loads(\n",
    "                user_comment_exif.decode(\"utf-8\").replace(\"\\'\", \"\\\"\")\n",
    "            )\n",
    "        \n",
    "        except UnicodeDecodeError as e:\n",
    "            raise ValueError(f\"Unable to decode exif data: {e}\")\n",
    "        \n",
    "        image_id = -1 if \"id\" not in exif_dict else int(exif_dict[\"id\"])\n",
    "        return {\n",
    "            \"height\": image.height,\n",
    "            \"width\": image.width,\n",
    "            \"lat\": exif_dict[\"lat\"],\n",
    "            \"long\": exif_dict[\"lng\"],\n",
    "            \"image_id\": image_id,\n",
    "            \"map_provider\": exif_dict[\"mapProvider\"]\n",
    "        }\n",
    "    \n",
    "    return spark.udf.register(\n",
    "        \"get_image_metadata_udf\",\n",
    "        get_image_metadata, \n",
    "        towerscout_image_metadata_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a923cdec-9789-4e22-bd44-f5aeb080b76e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog, schema = \"edav_dev_csels\", \"towerscout\"\n",
    "batch_size = 8\n",
    "num_workers = 20\n",
    "num_partitions = 6\n",
    "\n",
    "new_towerscout_inference_udf = new_make_towerscout_predict_udf(catalog, schema, yolo_alias=\"aws\", efficientnet_alias=\"aws\", batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "old_towerscout_inference_udf = old_make_towerscout_predict_udf(catalog, schema, batch_size=batch_size)\n",
    "old_image_metadata_udf = old_make_image_metadata_udf(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f409ad-5f76-426a-b2c9-86ad5e1819ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#base_path = f\"/Volumes/{catalog}/{schema}/images/maps/bronze/{user_id}/*\"\n",
    "base_path = f\"/Volumes/{catalog}/{schema}/images/maps/bronze/{user_id}/{request_id}\"\n",
    "\n",
    "image_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"binaryFile\")\n",
    "    .load(base_path) # parameterize\n",
    "    .select(\"content\", \"path\")\n",
    "    .repartition(num_partitions)\n",
    "    #.limit(200)\n",
    ")\n",
    "\n",
    "print(f\"Number of images: {image_df.count()}\")\n",
    "print(f\"Number of partitions: {image_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5dfdb49-e663-4327-b831-a7b0b4f662a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df_old = (\n",
    "    image_df\n",
    "    .transform(trf.perform_inference, old_towerscout_inference_udf)\n",
    "    .transform(trf.extract_metadata, old_image_metadata_udf)\n",
    ")\n",
    "\n",
    "display(transformed_df_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fb96d5-f299-409a-b337-866b93f4edf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df_new = (\n",
    "    image_df\n",
    "    .transform(trf.perform_inference, new_towerscout_inference_udf)\n",
    ")\n",
    "\n",
    "display(transformed_df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89813ecb-d7a5-4c08-8e8d-e368c2721db4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(dir(towerscout_inference_udf))\n",
    "# print(hasattr(towerscout_inference_udf, \"returnType\"))\n",
    "# print(towerscout_inference_udf.returnType == UDF_RETURN_TYPE)\n",
    "\n",
    "# print(isinstance(towerscout_inference_udf, Callable))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_inference_udf_optimize outer UDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
