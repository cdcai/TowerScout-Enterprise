{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b09f0e-76be-4f2f-930f-5b6cb8a6e328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from json import loads\n",
    "from typing import Any, TypedDict, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tsdb.ml.utils import cut_square_detection\n",
    "from models.common import Detections  # Detection object for YOLOv5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f175f2-a5b7-4e8d-80f3-66fbd97e9391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ImageMetadata(TypedDict):\n",
    "    \"\"\"\n",
    "    A class to represent image metadata.\n",
    "    height: the image height\n",
    "    width: the image width\n",
    "    lat: the latitude of the image\n",
    "    long: the longitude of the image\n",
    "    image_id: the id of the image\n",
    "    map_provider: the map provider the image is from \n",
    "    image: The PIL image object\n",
    "    \"\"\"\n",
    "    height: int\n",
    "    width: int\n",
    "    lat: float\n",
    "    long: float\n",
    "    image_id: int\n",
    "    map_provider: str\n",
    "    image: Image\n",
    "\n",
    "\n",
    "def get_image_metadata(image_binary: bytes) -> ImageMetadata:  # pragma: no cover\n",
    "        # Try to read the image and if we fail, we have to default to\n",
    "        # to the null image case\n",
    "        image_binary = BytesIO(image_binary)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_binary)\n",
    "            exif = image._getexif()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            exif = None\n",
    "        except UnicodeDecodeError:\n",
    "            exif = None\n",
    "\n",
    "        user_comment_exif_id = 37510\n",
    "\n",
    "        if exif is None or user_comment_exif_id not in exif:\n",
    "            # we need to return with default values\n",
    "            fake_image = Image.new('RGB', (640, 640), (0, 0, 0))\n",
    "            return {\n",
    "                \"height\": 640,\n",
    "                \"width\": 640,\n",
    "                \"lat\": 0.0,\n",
    "                \"long\": 0.0,\n",
    "                \"image_id\": -1,\n",
    "                \"map_provider\": \"unknown\",\n",
    "                \"image\": fake_image\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            user_comment_exif = exif[user_comment_exif_id]\n",
    "            exif_dict = loads(\n",
    "                user_comment_exif.decode(\"utf-8\").replace(\"\\'\", \"\\\"\")\n",
    "            )\n",
    "        \n",
    "        except UnicodeDecodeError as e:\n",
    "            # can we gracefully handle this?\n",
    "            raise ValueError(f\"Unable to decode exif data: {e}\")\n",
    "        \n",
    "        image_id = -1 if \"id\" not in exif_dict else int(exif_dict[\"id\"])\n",
    "        return {\n",
    "            \"height\": image.height,\n",
    "            \"width\": image.width,\n",
    "            \"lat\": exif_dict[\"lat\"],\n",
    "            \"long\": exif_dict[\"lng\"],\n",
    "            \"image_id\": image_id,\n",
    "            \"map_provider\": exif_dict[\"mapProvider\"],\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "\n",
    "class ImageBinaryDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index) -> ImageMetadata:\n",
    "         return get_image_metadata(self.images[index])\n",
    "    \n",
    "\n",
    "def inference_collate_fn(data: list[dict[str, ImageMetadata]]) -> dict[str, Union[Image,dict[str, Any]]]:\n",
    "    batch = defaultdict(list)\n",
    "    for item in data:\n",
    "        batch[\"images\"].append(item.pop(\"image\"))\n",
    "        batch[\"images_metadata\"].append(item)\n",
    "\n",
    "    return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e979a6-c02e-4040-b96c-065beffff209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "request_id = \"be69e91f\"\n",
    "user_id = \"cnu4\"\n",
    "base_path = f\"/Volumes/edav_dev_csels/towerscout/images/maps/bronze/{user_id}/{request_id}\"\n",
    "\n",
    "\n",
    "image_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"binaryFile\")\n",
    "    .load(base_path) # parameterize\n",
    "    .select(\"content\")\n",
    "    .limit(20)\n",
    "    #.repartition(8)\n",
    "    #.withColumn(\"inference\", yolo_inference_udf(F.col(\"content\")))\n",
    ")\n",
    "\n",
    "\n",
    "image_df = image_df.toPandas()\n",
    "image_bins = image_df[\"content\"]\n",
    "\n",
    "bin_dataset = ImageBinaryDataset(image_bins)\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "\n",
    "loader = DataLoader(\n",
    "        bin_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers,\n",
    "        collate_fn=inference_collate_fn \n",
    "    )\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d5f663c-ee4a-4529-b883-bf2620c43d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yolo_model = mlflow.pytorch.load_model(\n",
    "            model_uri=f\"models:/edav_dev_csels.towerscout.yolo_autoshape@aws\"\n",
    "        )\n",
    "\n",
    "en_model =  mlflow.pytorch.load_model(\n",
    "            model_uri=f\"models:/edav_dev_csels.towerscout.efficientnet@aws\"\n",
    "        )\n",
    "\n",
    "\n",
    "yolo_model.eval()\n",
    "en_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():  # pragma: no cover\n",
    "    en_model.cuda()\n",
    "    yolo_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d078b05-f95b-4111-9cd3-ed160b97bb71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_yolo_detections(\n",
    "    images: list[Image],\n",
    "    images_metadata: dict[str, Any],\n",
    "    yolo_results: Detections,\n",
    "    secondary_model: torch.nn.Module = None,\n",
    "    **kwargs\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    A function to parse the detections from the YOLO model by converting them into a list\n",
    "    of dicts with the following keys:\n",
    "    - x1: the x1 coordinate of the bounding box\n",
    "    - y1: the y1 coordinate of the bounding box\n",
    "    - x2: the x2 coordinate of the bounding box\n",
    "    - y2: the y2 coordinate of the bounding box\n",
    "    - conf: the YOLO model confidence of the detection\n",
    "    - class: the class of the detection\n",
    "    - class_name: the name of the class of the detection\n",
    "    - secondary: the secondary model confidence of the detection (if a secondary model is supplied)\n",
    "\n",
    "    Args:\n",
    "        images: the list of PIL images\n",
    "        images_metadata: the metadata for the images\n",
    "        yolo_results: the Detections object from the YOLO model\n",
    "        secondary_model: the secondary model used to evaluate the detections\n",
    "        **kwargs: additional keyword arguments to pass to the secondary model\n",
    "    Returns:\n",
    "        A list of dicts with the keys from above.\n",
    "    \"\"\"\n",
    "    parsed_results = []\n",
    "    batch_detections = yolo_results.xyxyn\n",
    "\n",
    "    for image, image_detections in zip(images, batch_detections):\n",
    "        image_detections = image_detections.cpu().numpy().tolist()\n",
    "        \n",
    "        if secondary_model is not None:\n",
    "            apply_secondary_model(secondary_model, image, image_detections, **kwargs)\n",
    "\n",
    "        image_results = [\n",
    "                    {\n",
    "                        \"x1\": item[0],\n",
    "                        \"y1\": item[1],\n",
    "                        \"x2\": item[2],\n",
    "                        \"y2\": item[3],\n",
    "                        \"conf\": item[4],\n",
    "                        \"class\": int(item[5]),\n",
    "                        \"class_name\": yolo_results.names[int(item[5])],\n",
    "                        \"secondary\": item[6] if len(item) > 6 else 1,\n",
    "                    }\n",
    "                    for item in image_detections\n",
    "                ]\n",
    "\n",
    "        parsed_results.append(image_results)\n",
    "\n",
    "    return parsed_results\n",
    "\n",
    "\n",
    "def apply_secondary_model(\n",
    "    secondary_model: torch.nn.Module,\n",
    "    image: Image,\n",
    "    detections: list[np.array],\n",
    "    min_conf: float = 0.25,\n",
    "    max_conf: float = 0.65,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A function to apply the secondary model to the detections from the YOLO model. The function \n",
    "    first crops the image based on the bounding box predicted by the YOLO model, then applies \n",
    "    the secondary model to the cropped image to determine the probablity the image contains a cooling tower\n",
    "    and appends the computed probability to the detection array.\n",
    "\n",
    "    Args:\n",
    "        secondary_model: the secondary model to apply to the cropped image\n",
    "        image: the image to crop\n",
    "        detections: list of the detections from the YOLO model for the input image\n",
    "        min_conf: the minimum confidence to apply the secondary model\n",
    "        max_conf: the maximum confidence to apply the secondary model\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize([456, 456]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.5553, 0.5080, 0.4960), std=(0.1844, 0.1982, 0.2017)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, conf = detection[0:5]\n",
    "\n",
    "        # Use secondary model only for certain confidence range\n",
    "        if conf >= min_conf and conf <= max_conf:\n",
    "            bbox_cropped_image = cut_square_detection(image, x1, y1, x2, y2)\n",
    "\n",
    "            # apply transformations\n",
    "            input = transform(bbox_cropped_image).unsqueeze(0)\n",
    "\n",
    "            if torch.cuda.is_available():  # pragma: no cover\n",
    "                input = input.cuda()\n",
    "\n",
    "            # subtract from 1 because the secondary has class 0 as tower\n",
    "            output = 1 - torch.sigmoid(secondary_model(input).cpu()).item()\n",
    "            p2 = output\n",
    "        elif conf < min_conf:\n",
    "            # set secondary classifier probability to 0\n",
    "            p2 = 0\n",
    "        else:\n",
    "            # if >= max_conf set secondary classifier probability to 1\n",
    "            p2 = 1\n",
    "\n",
    "        detection.append(p2)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "# for batch in loader:\n",
    "#     #print(\"INPUT BATCH:\\n\", batch['images'])\n",
    "#     #print(batch['image'][0].size())\n",
    "#     yolo_output = yolo_model(batch[\"images\"])\n",
    "#     print(f\"yolo output: {yolo_output.xyxyn}\")\n",
    "#     parsed_results = parse_yolo_detections(batch[\"images\"], batch[\"images_metadata\"], yolo_output, en_model, min_conf=0.65, max_conf=0.95)\n",
    "#     print(\"PARSED RESULTS:\\n\", parsed_results)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1c5e95-e673-4fd0-8a9b-c6f30807d3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_towerscout_predict_udf(\n",
    "    catalog: str,\n",
    "    schema: str,\n",
    "    yolo_alias: str = \"aws\",\n",
    "    efficientnet_alias: str = \"aws\",\n",
    "    batch_size: int = 100\n",
    ") -> DataFrame:  # pragma: no cover\n",
    "    \"\"\"\n",
    "    For a pandas UDF, we need the outer function to initialize the models\n",
    "    and the inner function to perform the inference process. For more\n",
    "    information, see the following reference by NVIDIA:\n",
    "    -\n",
    " \n",
    "    Args:\n",
    "        model_fn (InferenceModelType): The PyTorch model.\n",
    "        batch_size (int): Batch size for the DataLoader.\n",
    " \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with predictions.\n",
    "    \"\"\"\n",
    "    set_registry_uri(\"databricks-uc\")\n",
    " \n",
    "    yolo_model_name = f\"{catalog}.{schema}.yolo_autoshape\"\n",
    "    en_model_name = f\"{catalog}.{schema}.efficientnet\"  \n",
    " \n",
    "    # Retrieves models by alias and create inference objects\n",
    "    yolo_detector = mlflow.pytorch.load_model(\n",
    "            model_uri=f\"models:/{yolo_model_name}@{yolo_alias}\"\n",
    "        )\n",
    "    \n",
    "    YOLOv5_Detector.from_uc_registry(\n",
    "        model_name=yolo_model_name,\n",
    "        alias=yolo_alias,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    " \n",
    "    # We nearly always use efficientnet for classification but you don't have to\n",
    "    en_classifier = mlflow.pytorch.load_model(\n",
    "            model_uri=f\"models:/{en_model_name}@{efficientnet_alias}\"\n",
    "        )\n",
    " \n",
    "    metadata = {\n",
    "        \"yolo_model\": \"yolo_autoshape\",\n",
    "        \"yolo_model_version\": yolo_detector.uc_version,\n",
    "        \"efficientnet_model\": \"efficientnet\",\n",
    "        \"efficientnet_model_version\": en_classifier.uc_version,\n",
    "    }\n",
    " \n",
    "    return_type = T.StructType([\n",
    "        T.StructField(\"bboxes\", yolo_detector.return_type),\n",
    "        T.StructField(\"model_version\", MODEL_VERSION_STRUCT)\n",
    "    ])\n",
    " \n",
    "    @no_grad()\n",
    "    def predict(content_series_iter: pd.Series):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        This predict function is distributed across executors to perform inference.\n",
    " \n",
    "        YOLOv5 library expects the following image formats:\n",
    "        For size(height=640, width=1280), RGB images example inputs are:\n",
    "        #   file:        ims = 'data/images/zidane.jpg'  # str or PosixPath\n",
    "        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'\n",
    "        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)\n",
    "        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)\n",
    "        #   numpy:           = np.zeros((640,1280,3))  # HWC\n",
    "        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)\n",
    "        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n",
    "        - Source: https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    "       \n",
    "        The ultralytics lib accepts the following image formats:\n",
    "        - Source: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/engine/model.py\n",
    "       \n",
    " \n",
    "        # No need to resize for yolov5 lib as it does it for you\n",
    "        - Source: letterbox and exif_transpose funcs in:\n",
    "            https://github.com/ultralytics/yolov5/blob/master/models/common.py\n",
    " \n",
    "        Args:\n",
    "            content_series_iter: Iterator over content series.\n",
    " \n",
    "        Yields:\n",
    "            DataFrame: DataFrame with predicted labels.\n",
    "        \"\"\"\n",
    "        for content_series in content_series_iter:\n",
    "            # Create dataset object to apply transformations\n",
    "            image_batch = [\n",
    "                Image.open(BytesIO(content)).convert(\"RGB\")\n",
    "                for content in content_series\n",
    "            ]\n",
    " \n",
    "            # Perform inference on batch\n",
    "            outputs = yolo_detector.predict(\n",
    "                model_input=image_batch,\n",
    "                secondary=en_classifier\n",
    "            )\n",
    " \n",
    "            outputs = [\n",
    "                {\"bboxes\": output, \"model_version\": metadata}\n",
    "                for output in outputs\n",
    "            ]\n",
    "            yield pd.DataFrame(outputs)\n",
    " \n",
    "    return pandas_udf(return_type, PandasUDFType.SCALAR_ITER)(predict)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_inference_udf_optimize",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
