"""
This module contains utilities for extending Spark Structured Streams
"""
from datetime import datetime, timedelta
import threading, queue, json

from pyspark.sql import SparkSession
from pyspark.sql.streaming import StreamingQuery, StreamingQueryListener
import pyspark.sql.streaming.listener as listener


class StreamShutdownListener(StreamingQueryListener):
    """
    A listener that shuts down a spark stream after a certain number of minutes.

    To use this object:
        listener = StreamShutdownListener(timeout=1)
        spark.streams.addListener(listener)

        stream_job = spark.readStream().format("rate").start()
        listener.set_stream(stream_job)
        stream_job.awaitTermination()

    Args:
        minutes: Number of minutes to shut stream off
    """
    def __init__(self, minutes: int):
        self.max_duration = timedelta(minutes=timeout)
        self._idle_start_time = None
        self._stream = None
        self._lock = threading.Lock()
    
    @staticmethod
    def utc_string_to_datetime(date: str) -> datetime:
        """
        Converts a UTC string to a datetime object.
        """
        return datetime.strptime(date, "%Y-%m-%dT%H:%M:%S.%fZ")

    def onQueryStarted(self, event: listener.QueryStartedEvent) -> None:
        pass

    def onQueryIdle(self, event: listener.QueryIdleEvent) -> None:
        """
        Manages the countdown to stream shutdown. If no idle time exists, sets it to the current time.
        Otherwise, compares current time to the set idle time. If the timedelta value is reached, stops the stream.
        """
        current_time = self.utc_string_to_datetime(event.timestamp)
        
        if self._idle_start_time is None:
            with self._lock:
                self._idle_start_time = current_time
        
        if (current_time - self._idle_start_time) > self.max_duration:
            self._stream.stop()

    def onQueryProgress(self, event: listener.QueryProgressEvent) -> None:
        """
        If the query is making progress, we want to reset the idle time so progress
        can continue. This makes the shutdown more graceful.
        """
        if self._idle_start_time is not None:
            with self._lock:
                self._idle_start_time = None
    
    def onQueryTerminated(self, event: listener.QueryTerminatedEvent) -> None:
        pass

    def set_stream(self, stream: StreamingQuery) -> None:
        """
        Adds a query object to the listener that will be shut down when max_duration is reached.
        """
        self._stream = stream


class StreamLogger:
    """
    This class defines a separate thread to perform logging via a queue. The goal
    is to have the listener write to a queue and a separate thread to read from the queue.

    Args:
        spark: SparkSession object
        log_path: Path to write logs to. Log file name is generated by the current time
        job_id: Job ID (can be None if ran from a notebook)
    """ 
    def __init__(self, log_path: str, job_id: str) -> None:
        self.spark = spark
        self.job_id = job_id

        # Create Log File
        log_file_name = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        self.log_path = f"{log_path}/{log_file_name}.json"
        self.log_queue = queue.Queue()
        self.listener = self._create_listener()

        # Start background thread
        self._writer_thread = threading.Thread(target=self._log_writer, daemon=True)
        self._writer_thread.start()
    
    def _create_listener(self):
        """
        Creates a listener object that writes to a queue. Spark recommends logging
        listeners to do minimal processing to prevent blocking the main threads.
        """
        log_queue = self.log_queue
        job_id = self.job_id

        class StreamLoggingListener(StreamingQueryListener):
            """
            A listener that writes to a queue. Spark recommends logging listeners to do minimal processing to prevent blocking the main threads.
            """
            def onQueryStarted(self, event: listener.QueryStartedEvent) -> None:
                """
                Writes the starting event to the queue, including metadata about the stream
                """
                log_entry = {
                    "event": "query_started",
                    "id": str(event.id),
                    "runId": str(event.runId),
                    "name": event.name or "unknown",
                    "timestamp": event.timestamp,
                    "jobId": job_id,
                }
                log_queue.put(json.dumps(log_entry))

            def onQueryProgress(self, event: listener.QueryProgressEvent) -> None:
                """
                Writes the progress event to the queue
                """
                log_queue.put(event.progress.json)
            
            def onQueryTerminated(self, event: listener.QueryTerminatedEvent) -> None:
                """
                Writes the termination event to the queue, including any exception that
                may have been raised
                """
                log_entry = {
                    "event": "query_terminated",
                    "id": str(event.id),
                    "runId": str(event.runId),
                    "exception": event.exception or None,
                    "jobId": job_id,
                }

                log_queue.put(json.dumps(log_entry))
                log_queue.put(None)

        return StreamLoggingListener()
    
    def _log_writer(self):
        """
        Writes the queue to a log file. This is a separate thread to prevent blocking the main thread.
        task_done() is used to ensure the queue is empty before exiting the thread.
        """
        with open(self.log_path, "a") as log_file:
            while True:
                event_json = self.log_queue.get()
                if event_json is None:
                    break

                log_file.write(event_json + "\n")
                log_file.flush()
                self.log_queue.task_done()
    
    def stop(self):
        """
        Stops the logging thread and removes the listener.
        """
        self.log_queue.put(None)
        self._writer_thread.join()
